{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 0. Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchtext, datasets, math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Dataset Acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TinyStories Dataset Overview\n",
    "\n",
    "- Description: A Dataset containing synthetically generated (by GPT-3.5 and GPT-4) short stories that only use a small vocabulary.\n",
    "\n",
    "- Described in the following paper: https://arxiv.org/abs/2305.07759.\n",
    "\n",
    "- Size: ~50k stories.\n",
    "\n",
    "- Number of rows: 2,141,709"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TinyStories dataset\n",
    "dataset = datasets.load_dataset('roneneldan/TinyStories', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 2119719\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 21990\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 2119719\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 10995\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 10995\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Split the validation set into validation and test sets\n",
    "validation_test_split = dataset['validation'].train_test_split(test_size=0.5, seed=123)\n",
    "\n",
    "# Create a new DatasetDict with train, validation, and test splits\n",
    "dataset = datasets.DatasetDict({\n",
    "    'train': dataset['train'],\n",
    "    'validation': validation_test_split['train'],\n",
    "    'test': validation_test_split['test']\n",
    "})\n",
    "\n",
    "# Print the new dataset structure\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2119719, 1)\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detail the steps taken to preprocess the text data. (1 points)\n",
    "\n",
    "1. Load the Dataset<br><br>\n",
    "\n",
    "2. Define the Tokenizer\n",
    "- Use `torchtext.data.utils.get_tokenizer` to create a tokenizer.<br><br>\n",
    "\n",
    "3. Tokenize the Text\n",
    "- Define a function to tokenize each example in the dataset.\n",
    "- Use the `map` method to apply the tokenization function to the entire dataset.<br><br>\n",
    "\n",
    "4. Remove Unnecessary Columns\n",
    "- Remove the original `'text'` column after tokenization.<br><br>\n",
    "\n",
    "5. Build Vocabulary and Numericalize Data\n",
    "- Build a vocabulary using `torchtext.vocab.build_vocab_from_iterator`.\n",
    "  - Set a minimum frequency threshold (e.g., `min_freq=3`) to filter out rare tokens.\n",
    "- Add special tokens (e.g., `'<unk>'` and `'<eos>'`) if they don't already exist in the vocabulary. <br><br>\n",
    "\n",
    "6. Inspect the Results\n",
    "- Verify that the tokenization was applied correctly by inspecting the tokenized dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing\n",
    "\n",
    "Tokenize the sentences to text tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tokenizer\n",
    "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
    "\n",
    "# define a lambda function to tokenize each example in the dataset:\n",
    "tokenize_data = lambda example, tokenizer: {'tokens': tokenizer(example['text'])}\n",
    "\n",
    "# Apply Tokenization to the Datase\n",
    "tokenized_dataset = dataset.map(tokenize_data, remove_columns=['text'], fn_kwargs={'tokenizer': tokenizer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 2119719\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 10995\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 10995\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['once', 'upon', 'a', 'time', ',', 'there', 'was', 'a', 'little', 'boy', 'named', 'tom', '.', 'tom', 'loved', 'to', 'look', 'up', 'at', 'the', 'sky', 'every', 'night', '.', 'he', 'would', 'often', 'see', 'a', 'comet', 'speeding', 'by', 'and', 'it', 'would', 'always', 'make', 'him', 'happy', '.', 'one', 'night', ',', 'tom', 'was', 'a', 'bit', 'worried', 'because', 'he', 'couldn', \"'\", 't', 'see', 'the', 'comet', '.', 'he', 'asked', 'his', 'mom', ',', 'mom', ',', 'where', 'is', 'the', 'comet', '?', 'why', 'can', \"'\", 't', 'i', 'see', 'it', '?', 'his', 'mom', 'smiled', 'warmly', 'and', 'said', ',', 'don', \"'\", 't', 'worry', ',', 'tom', '.', 'it', 'will', 'rise', 'again', 'soon', '.', 'tom', 'was', 'confused', '.', 'he', 'asked', ',', 'what', 'does', 'that', 'mean', '?', 'his', 'mom', 'explained', ',', 'it', 'means', 'that', 'it', 'will', 'appear', 'again', '.', 'everyone', 'and', 'everything', 'has', 'their', 'ups', 'and', 'downs', ',', 'just', 'like', 'the', 'comet', '.', 'just', 'be', 'patient', 'and', 'it', 'will', 'come', 'back', 'to', 'us', '.', 'tom', 'nodded', 'and', 'looked', 'up', 'at', 'the', 'night', 'sky', '.', 'he', 'was', 'no', 'longer', 'worried', '.', 'he', 'knew', 'that', 'everything', 'will', 'come', 'back', 'even', 'if', 'it', 'seems', 'to', 'be', 'gone', 'for', 'a', 'while', '.', 'tom', 'had', 'learned', 'a', 'valuable', 'lesson', 'from', 'his', 'mom', 'don', \"'\", 't', 'worry', 'even', 'if', 'something', 'is', 'gone', 'for', 'a', 'while', '.', 'it', 'will', 'rise', 'again', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset['train'][223]['tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numericalizing\n",
    "\n",
    "We will tell torchtext to add any word that has occurred at least three times in the dataset to the vocabulary because otherwise it would be too big.  Also we shall make sure to add `unk` and `eos`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = torchtext.vocab.build_vocab_from_iterator(tokenized_dataset['train']['tokens'], min_freq=3)\n",
    "vocab.insert_token('<unk>', 0)\n",
    "vocab.insert_token('<eos>', 1)\n",
    "vocab.set_default_index(vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34614\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<eos>', '.', 'the', 'and', ',', 'to', 'a', 'was', 'he']\n"
     ]
    }
   ],
   "source": [
    "print(vocab.get_itos()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the batch loader\n",
    "\n",
    "### Prepare data\n",
    "\n",
    "Given \"Chaky loves eating at AIT\", and \"I really love deep learning\", and given batch size = 3, we will get three batches of data \"Chaky loves eating at\", \"AIT `<eos>` I really\", \"love deep learning `<eos>`\".  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(dataset, vocab, batch_size):\n",
    "    data = []\n",
    "    for example in dataset:\n",
    "        if example['tokens']:\n",
    "            tokens = example['tokens'].append('<eos>')\n",
    "            tokens = [vocab[token] for token in example['tokens']]\n",
    "            data.extend(tokens)\n",
    "    data = torch.LongTensor(data)\n",
    "    num_batches = data.shape[0] // batch_size\n",
    "    data = data[:num_batches * batch_size]\n",
    "    data = data.view(batch_size, num_batches) #view vs. reshape (whether data is contiguous)\n",
    "    return data #[batch size, seq len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_data = get_data(tokenized_dataset['train'], vocab, batch_size)\n",
    "valid_data = get_data(tokenized_dataset['validation'], vocab, batch_size)\n",
    "test_data  = get_data(tokenized_dataset['test'],  vocab, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 3446557])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe the model architecture and the training process. (1 points)\n",
    "\n",
    "**Model Architecture**\n",
    "\n",
    "The model is an **LSTM-based Language Model** implemented in PyTorch. It consists of the following key components:\n",
    "\n",
    "1. **Embedding Layer**:\n",
    "   - Converts token indices into dense vectors of size `emb_dim`.\n",
    "   - Input: `[batch_size, seq_len]`\n",
    "   - Output: `[batch_size, seq_len, emb_dim]`\n",
    "\n",
    "2. **LSTM Layer**:\n",
    "   - Processes the sequence of embeddings to capture contextual information.\n",
    "   - Input: `[batch_size, seq_len, emb_dim]`\n",
    "   - Output:\n",
    "     - `output`: `[batch_size, seq_len, hid_dim]` (hidden states for each time step)\n",
    "     - `hidden`: `[num_layers, batch_size, hid_dim]` (final hidden state and cell state)\n",
    "\n",
    "3. **Dropout Layer**:\n",
    "   - Applied after the embedding and LSTM layers to prevent overfitting.\n",
    "\n",
    "4. **Fully Connected Layer**:\n",
    "   - Maps the LSTM output to the vocabulary size (`vocab_size`).\n",
    "   - Input: `[batch_size, seq_len, hid_dim]`\n",
    "   - Output: `[batch_size, seq_len, vocab_size]`\n",
    "\n",
    "5. **Initialization**:\n",
    "   - Weights are initialized uniformly to small values for stable training.\n",
    "\n",
    "\n",
    "**Training Process**\n",
    "\n",
    "### Data Preparation\n",
    "1. **Tokenization and Numericalization**:\n",
    "   - Text data is tokenized and converted into integer indices using a vocabulary.\n",
    "   - Special tokens like `<eos>` (end of sequence) are added.\n",
    "\n",
    "2. **Batching**:\n",
    "   - Numericalized data is split into batches of size `batch_size`.\n",
    "   - Each batch is divided into sequences of length `seq_len`.\n",
    "\n",
    "### Training Loop\n",
    "1. **Initialization**:\n",
    "   - The LSTM hidden state is initialized to zeros at the start of each epoch.\n",
    "\n",
    "2. **Forward Pass**:\n",
    "   - The model predicts the next token for each position in the sequence.\n",
    "   - The output is reshaped for the loss function.\n",
    "\n",
    "3. **Loss Calculation**:\n",
    "   - **CrossEntropyLoss** computes the difference between predicted and actual tokens.\n",
    "   - Loss is averaged over the sequence length.\n",
    "\n",
    "4. **Backpropagation**:\n",
    "   - Gradients are computed and clipped to prevent exploding gradients.\n",
    "   - The optimizer updates the model parameters.\n",
    "\n",
    "5. **Evaluation**:\n",
    "   - The model is evaluated on the validation set after each epoch.\n",
    "   - The learning rate is adjusted using a scheduler if the validation loss plateaus.\n",
    "\n",
    "6. **Checkpointing**:\n",
    "   - The model with the best validation loss is saved.\n",
    "\n",
    "\n",
    "## Hyperparameters\n",
    "- **`vocab_size`**: Size of the vocabulary.(9879)\n",
    "- **`emb_dim`**: Dimension of the embedding layer (1024).\n",
    "- **`hid_dim`**: Dimension of the LSTM hidden state (1024).\n",
    "- **`num_layers`**: Number of LSTM layers (2).\n",
    "- **`dropout_rate`**: Dropout probability (0.65).\n",
    "- **`batch_size`**: Number of sequences per batch (128).\n",
    "- **`seq_len`**: Length of each sequence (50).\n",
    "- **`clip`**: Gradient clipping threshold (0.25).\n",
    "- **`lr`**: Learning rate (1e-3).\n",
    "- **`n_epochs`**: Number of training epochs (50).\n",
    "\n",
    "---\n",
    "\n",
    "## Metrics\n",
    "- **Perplexity**:\n",
    "  - Used to evaluate the model's performance.\n",
    "  - Defined as `exp(loss)`.\n",
    "  - Lower perplexity indicates better performance.\n",
    "\n",
    "---\n",
    "\n",
    "## Training Output\n",
    "During training, the following metrics are printed for each epoch:\n",
    "- **Train Perplexity**: Perplexity on the training set.\n",
    "- **Valid Perplexity**: Perplexity on the validation set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hid_dim, num_layers, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hid_dim    = hid_dim\n",
    "        self.emb_dim    = emb_dim\n",
    "        \n",
    "        self.embedding  = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm       = nn.LSTM(emb_dim, hid_dim, num_layers=num_layers, dropout=dropout_rate, batch_first=True)\n",
    "        self.dropout    = nn.Dropout(dropout_rate)\n",
    "        self.fc         = nn.Linear(hid_dim, vocab_size)\n",
    "        \n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        init_range_emb = 0.1\n",
    "        init_range_other = 1/math.sqrt(self.hid_dim)\n",
    "        self.embedding.weight.data.uniform_(-init_range_emb, init_range_other)\n",
    "        self.fc.weight.data.uniform_(-init_range_other, init_range_other)\n",
    "        self.fc.bias.data.zero_()\n",
    "        for i in range(self.num_layers):\n",
    "            self.lstm.all_weights[i][0] = torch.FloatTensor(self.emb_dim,\n",
    "                self.hid_dim).uniform_(-init_range_other, init_range_other) #We\n",
    "            self.lstm.all_weights[i][1] = torch.FloatTensor(self.hid_dim,   \n",
    "                self.hid_dim).uniform_(-init_range_other, init_range_other) #Wh\n",
    "    \n",
    "    def init_hidden(self, batch_size, device):\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n",
    "        cell   = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n",
    "        return hidden, cell\n",
    "        \n",
    "    def detach_hidden(self, hidden):\n",
    "        hidden, cell = hidden\n",
    "        hidden = hidden.detach() #not to be used for gradient computation\n",
    "        cell   = cell.detach()\n",
    "        return hidden, cell\n",
    "        \n",
    "    def forward(self, src, hidden):\n",
    "        #src: [batch_size, seq len]\n",
    "        embedding = self.dropout(self.embedding(src)) #harry potter is\n",
    "        #embedding: [batch-size, seq len, emb dim]\n",
    "        output, hidden = self.lstm(embedding, hidden)\n",
    "        #ouput: [batch size, seq len, hid dim]\n",
    "        #hidden: [num_layers * direction, seq len, hid_dim]\n",
    "        output = self.dropout(output)\n",
    "        prediction =self.fc(output)\n",
    "        #prediction: [batch_size, seq_len, vocab_size]\n",
    "        return prediction, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training \n",
    "\n",
    "Follows very basic procedure.  One note is that some of the sequences that will be fed to the model may involve parts from different sequences in the original dataset or be a subset of one (depending on the decoding length). For this reason we will reset the hidden state every epoch, this is like assuming that the next batch of sequences is probably always a follow up on the previous in the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "emb_dim = 1024                # 400 in the paper\n",
    "hid_dim = 1024                # 1150 in the paper\n",
    "num_layers = 2                # 3 in the paper\n",
    "dropout_rate = 0.65              \n",
    "lr = 1e-3                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 87,717,686 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "model      = LSTMLanguageModel(vocab_size, emb_dim, hid_dim, num_layers, dropout_rate).to(device)\n",
    "optimizer  = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion  = nn.CrossEntropyLoss()\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'The model has {num_params:,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data, seq_len, idx):\n",
    "    #data #[batch size, bunch of tokens]\n",
    "    src    = data[:, idx:idx+seq_len]                   \n",
    "    target = data[:, idx+1:idx+seq_len+1]  #target simply is ahead of src by 1            \n",
    "    return src, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, optimizer, criterion, batch_size, seq_len, clip, device):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    # drop all batches that are not a multiple of seq_len\n",
    "    # data #[batch size, seq len]\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches -1) % seq_len]  #we need to -1 because we start at 0\n",
    "    num_batches = data.shape[-1]\n",
    "    \n",
    "    #reset the hidden every epoch\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "    \n",
    "    for idx in tqdm(range(0, num_batches - 1, seq_len), desc='Training: ',leave=False):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #hidden does not need to be in the computational graph for efficiency\n",
    "        hidden = model.detach_hidden(hidden)\n",
    "\n",
    "        src, target = get_batch(data, seq_len, idx) #src, target: [batch size, seq len]\n",
    "        src, target = src.to(device), target.to(device)\n",
    "        batch_size = src.shape[0]\n",
    "        prediction, hidden = model(src, hidden)               \n",
    "\n",
    "        #need to reshape because criterion expects pred to be 2d and target to be 1d\n",
    "        prediction = prediction.reshape(batch_size * seq_len, -1)  #prediction: [batch size * seq len, vocab size]  \n",
    "        target = target.reshape(-1)\n",
    "        loss = criterion(prediction, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * seq_len\n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data, criterion, batch_size, seq_len, device):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    model.eval()\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches -1) % seq_len]\n",
    "    num_batches = data.shape[-1]\n",
    "\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx in range(0, num_batches - 1, seq_len):\n",
    "            hidden = model.detach_hidden(hidden)\n",
    "            src, target = get_batch(data, seq_len, idx)\n",
    "            src, target = src.to(device), target.to(device)\n",
    "            batch_size= src.shape[0]\n",
    "\n",
    "            prediction, hidden = model(src, hidden)\n",
    "            prediction = prediction.reshape(batch_size * seq_len, -1)\n",
    "            target = target.reshape(-1)\n",
    "\n",
    "            loss = criterion(prediction, target)\n",
    "            epoch_loss += loss.item() * seq_len\n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will be using a `ReduceLROnPlateau` learning scheduler which decreases the learning rate by a factor, if the loss don't improve by a certain epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 12.258\n",
      "\tValid Perplexity: 7.973\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1  # Train for only 1 epoch\n",
    "seq_len  = 50 #<----decoding length\n",
    "clip    = 0.25\n",
    "\n",
    "# Learning rate scheduler\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=0)\n",
    "\n",
    "# Track the best validation loss\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    # Train the model for 1 epoch\n",
    "    train_loss = train(model, train_data, optimizer, criterion, \n",
    "                batch_size, seq_len, clip, device)\n",
    "    \n",
    "    # Evaluate the model on the validation set\n",
    "    valid_loss = evaluate(model, valid_data, criterion, batch_size, \n",
    "                seq_len, device)\n",
    "\n",
    "    # Adjust the learning rate based on validation loss\n",
    "    lr_scheduler.step(valid_loss)\n",
    "\n",
    "    # Save the model if it achieves the best validation loss\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'best-val-lstm_lm_v2.pt')\n",
    "\n",
    "    # Print training and validation perplexity\n",
    "    print(f'\\tTrain Perplexity: {math.exp(train_loss):.3f}')\n",
    "    print(f'\\tValid Perplexity: {math.exp(valid_loss):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training took a whooping 4 Hours and 14 mins for 1 epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Perplexity: 8.008\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('best-val-lstm_lm_v2.pt',  map_location=device))\n",
    "test_loss = evaluate(model, test_data, criterion, batch_size, seq_len, device)\n",
    "print(f'Test Perplexity: {math.exp(test_loss):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-world inference\n",
    "\n",
    "Here we take the prompt, tokenize, encode and feed it into the model to get the predictions.  We then apply softmax while specifying that we want the output due to the last word in the sequence which represents the prediction for the next word.  We divide the logits by a temperature value to alter the model’s confidence by adjusting the softmax probability distribution.\n",
    "\n",
    "Once we have the Softmax distribution, we randomly sample it to make our prediction on the next word. If we get <unk> then we give that another try.  Once we get <eos> we stop predicting.\n",
    "    \n",
    "We decode the prediction back to strings last lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, max_seq_len, temperature, model, tokenizer, vocab, device, seed=None):\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "    model.eval()\n",
    "    tokens = tokenizer(prompt)\n",
    "    indices = [vocab[t] for t in tokens]\n",
    "    batch_size = 1\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(max_seq_len):\n",
    "            src = torch.LongTensor([indices]).to(device)\n",
    "            prediction, hidden = model(src, hidden)\n",
    "            \n",
    "            #prediction: [batch size, seq len, vocab size]\n",
    "            #prediction[:, -1]: [batch size, vocab size] #probability of last vocab\n",
    "            \n",
    "            probs = torch.softmax(prediction[:, -1] / temperature, dim=-1)  \n",
    "            prediction = torch.multinomial(probs, num_samples=1).item()    \n",
    "            \n",
    "            while prediction == vocab['<unk>']: #if it is unk, we sample again\n",
    "                prediction = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "            if prediction == vocab['<eos>']:    #if it is eos, we stop\n",
    "                break\n",
    "\n",
    "            indices.append(prediction) #autoregressive, thus output becomes input\n",
    "\n",
    "    itos = vocab.get_itos()\n",
    "    tokens = [itos[i] for i in indices]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "harry potter is a good animal . â€ he thanked him and hopped away . the end .\n",
      "\n",
      "0.7\n",
      "harry potter is a great animal . and he has lots of new friends .\n",
      "\n",
      "0.75\n",
      "harry potter is a great animal . and he has lots of new friends .\n",
      "\n",
      "0.8\n",
      "harry potter is a great animal . yeah , we are wealthy ! said his brother , giving the rabbit a big hug . bobby was so proud of his new discovery .\n",
      "\n",
      "1.0\n",
      "harry potter is on the other side of the field . would guide us to the top of the hill ? peace are the fastest . every year , letâ€™s go and explore\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = 'Harry Potter is '\n",
    "max_seq_len = 30\n",
    "seed = 0\n",
    "\n",
    "#smaller the temperature, more diverse tokens but comes \n",
    "#with a tradeoff of less-make-sense sentence\n",
    "temperatures = [0.5, 0.7, 0.75, 0.8, 1.0]\n",
    "for temperature in temperatures:\n",
    "    generation = generate(prompt, max_seq_len, temperature, model, tokenizer, \n",
    "                          vocab, device, seed)\n",
    "    print(str(temperature)+'\\n'+' '.join(generation)+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
